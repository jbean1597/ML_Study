# Word Embedding in PyTorch - Word2Vec and GloVe

After building a MLP with backprop from scratch, the next step I took was to begin word embeddings (specifically word2vec and GloVe). Taking the first step into Natural Language Processing with word2vec and GloVe is a large one since the data is textual instead of numerical. With word2vec text data get converted into a vector and, within the dimensions of the model, finds correlations and correctly weights the words. 

### Sources of Information

[PyTorch Ngrams Example](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)<br>
[Video Walkthrough of PyTorch Ngrams Example](https://www.youtube.com/watch?v=mCvW_qNm7rY)<br>
[PyTorch Dataset and DataLoader Docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)<br>
[PyTorch DataLoader Video](https://www.youtube.com/watch?v=PXOzkkB5eH0)<br>
